{"version":3,"sources":["binding.ts"],"names":["Onnxruntime","NativeModules","binding"],"mappings":";;;;;;;AAIA;;AAJA;AACA;AAgFA;AACA,MAAM;AAAEA,EAAAA;AAAF,IAAkBC,0BAAxB;AACO,MAAMC,OAAO,GAAGF,WAAhB","sourcesContent":["// Copyright (c) Microsoft Corporation. All rights reserved.\n// Licensed under the MIT License.\n\nimport type { InferenceSession } from 'onnxruntime-common';\nimport { NativeModules } from 'react-native';\n\n/**\n * model loading information\n */\ninterface ModelLoadInfo {\n  /**\n   * Key for an instance of InferenceSession, which is passed to run() function as parameter.\n   */\n  readonly key: string;\n\n  /**\n   * Get input names of the loaded model.\n   */\n  readonly inputNames: string[];\n\n  /**\n   * Get output names of the loaded model.\n   */\n  readonly outputNames: string[];\n}\n\n/**\n * Tensor type for react native, which doesn't allow ArrayBuffer, so data will be encoded as Base64 string.\n */\ninterface EncodedTensor {\n  /**\n   * the dimensions of the tensor.\n   */\n  readonly dims: readonly number[];\n  /**\n   * the data type of the tensor.\n   */\n  readonly type: string;\n  /**\n   * the Base64 encoded string of the buffer data of the tensor.\n   * if data is string array, it won't be encoded as Base64 string.\n   */\n  readonly data: string | string[];\n}\n\n/**\n * Binding exports a simple synchronized inference session object wrap.\n */\nexport declare namespace Binding {\n  type ModelLoadInfoType = ModelLoadInfo;\n  type EncodedTensorType = EncodedTensor;\n\n  type SessionOptions = InferenceSession.SessionOptions;\n  type RunOptions = InferenceSession.RunOptions;\n\n  type FeedsType = {\n    [name: string]: EncodedTensor;\n  };\n\n  // SessionHanlder FetchesType is different from native module's one.\n  // It's because Java API doesn't support preallocated output values.\n  type FetchesType = string[];\n\n  type ReturnType = {\n    [name: string]: EncodedTensor;\n  };\n\n  interface InferenceSession {\n    loadModel(\n      modelPath: string,\n      options: SessionOptions\n    ): Promise<ModelLoadInfoType>;\n    run(\n      key: string,\n      feeds: FeedsType,\n      fetches: FetchesType,\n      options: RunOptions\n    ): Promise<ReturnType>;\n  }\n}\n\n// export native binding\nconst { Onnxruntime } = NativeModules;\nexport const binding = Onnxruntime as Binding.InferenceSession;\n"]}